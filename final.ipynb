{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from numpy import random\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.pipeline import Pipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "metadata": {
        "id": "YIp2-VDofA7E"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "a21hMPGH4atU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "VZNy9J_nwRU3",
        "outputId": "229e53aa-8c80-4b12-9d46-56a73d144d4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def function_1(x):\n",
        "\n",
        "\n",
        "    median_imputer = joblib.load('/content/drive/MyDrive/median_impute.pkl')\n",
        "\n",
        "    # here we are loading the logistic regression model \n",
        "    #model = joblib.load('model.pkl')\n",
        "\n",
        "    # here we are giing to replace the na value to np.NaN value \n",
        "    \n",
        "    x = x.replace('na', np.NaN)\n",
        "    \n",
        "\n",
        "    # here we droping the those feature name which have more than 75% of missing value and which data have std_daviation is zero\n",
        "    x = x.drop(['cd_000', 'br_000', 'bq_000', 'bp_000', 'bq_000', 'bo_000','ab_000','cr_000'], axis = 1)\n",
        "    \n",
        "    # here we are manualy selecting the those feature name which have less than 15% missing values\n",
        "    median_data = ['aa_000', 'ac_000','ae_000','af_000','ag_001','ag_002','ag_003','ag_004','ag_005','ag_006','ag_007','ag_008','ag_009','ah_000','ai_000','aj_000','ak_000','al_000','am_0','an_000','ao_000','ap_000','aq_000','ar_000','as_000','at_000','au_000','av_000','ax_000','ay_000','ay_001','ay_002','ay_003','ay_004','ay_005','ay_006','ay_007','ay_008','ay_009','az_000','az_001','az_002','az_003','az_004','az_005','az_006','az_007','az_008','az_009','ba_000','ba_001','ba_002','ba_003','ba_004','ba_005','ba_006','ba_007','ba_008','ba_009','bb_000','bc_000','bd_000','be_000','bf_000','bg_000','bh_000','bi_000','bj_000','bs_000','bt_000','bu_000','bv_000','bx_000','by_000','bz_000','ca_000','cc_000','ce_000','ci_000','cj_000','ck_000','cn_000','cn_001','cn_002','cn_003','cn_004','cn_005','cn_006','cn_007','cn_008','cn_009','cp_000','cq_000','cs_000','cs_001','cs_002','cs_003','cs_004','cs_005','cs_006','cs_007','cs_008','cs_009','dd_000','de_000','df_000','dg_000','dh_000','di_000','dj_000','dk_000','dl_000','dm_000','dn_000','do_000','dp_000','dq_000','dr_000','ds_000','dt_000','du_000','dv_000','dx_000','dy_000','dz_000','ea_000','eb_000','ee_000','ee_001','ee_002','ee_003','ee_004','ee_005','ee_006','ee_007','ee_008','ee_009','ef_000','eg_000']\n",
        "    \n",
        "    # here we are selecting the those feature which have less than 75% and more than 15% missing values \n",
        "    model_data = ['ad_000', 'bk_000', 'bl_000', 'bm_000', 'bn_000', 'cf_000', 'cg_000','ch_000', 'cl_000', 'cm_000', 'co_000', 'ct_000', 'cu_000', 'cv_000','cx_000', 'cy_000', 'cz_000', 'da_000', 'db_000', 'dc_000', 'ec_00','ed_000']\n",
        "    \n",
        "    # here we are seprating those feature which are going to impute by median_imputation from data set\n",
        "    median_df = x.filter(median_data)\n",
        "\n",
        "    # here we are seperating the model_impute feature\n",
        "    model_df = x.filter(model_data)\n",
        "\n",
        "    median_imp = median_imputer.fit_transform(median_df)\n",
        "    \n",
        "    # making the data frame \n",
        "    median_imp_df = pd.DataFrame(median_imp, columns= median_df.columns)\n",
        "    \n",
        "    # KNN imputation \n",
        "    imputer = KNNImputer()\n",
        "    model_imp_test = imputer.fit_transform(model_df)\n",
        "    model_imp_df = pd.DataFrame(model_imp_test, columns= model_df.columns)\n",
        "    \n",
        "    # here we are concatinating the median_imputed dataframe and model_imputed data\n",
        "    data_df = pd.concat((median_imp_df, model_imp_df), axis = 1)\n",
        "\n",
        "    scalar =StandardScaler()\n",
        "    scalar.fit(data_df)\n",
        "    scal_data = scalar.transform(data_df)\n",
        "    scaler_data = pd.DataFrame(scal_data, columns = data_df.columns)\n",
        "\n",
        "    pca = PCA(n_components= 0.95)\n",
        "    pca_df = pca.fit_transform(scaler_data)\n",
        "    pca_df = pd.DataFrame(pca_df)\n",
        "\n",
        "    final_df = pd.concat((pca_df, scaler_data), axis = 1)\n",
        "\n",
        "    model = joblib.load('/content/drive/MyDrive/model.pkl')\n",
        "    y = model.predict(final_df)\n",
        "\n",
        "    return y\n",
        "\n"
      ],
      "metadata": {
        "id": "Edjnm4snkOmD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def function_2(x, y):\n",
        "\n",
        "  median_imputer = joblib.load('/content/drive/MyDrive/median_impute.pkl')\n",
        "\n",
        "    # here we are loading the logistic regression model \n",
        "    #model = joblib.load('model.pkl')\n",
        "\n",
        "    # here we are giing to replace the na value to np.NaN value\n",
        "   # here we are neg and pos with 0 a\n",
        "  x = x.replace('na', np.NaN)\n",
        "  #remap = {'neg':0, 'pos': 1}\n",
        "  #x = x.replace(remap)\n",
        "  #y = x['class']\n",
        "\n",
        "    # here we droping the those feature name which have more than 75% of missing value and which data have std_daviation is zero\n",
        "  x = x.drop(['cd_000', 'br_000', 'bq_000', 'bp_000', 'bq_000', 'bo_000','ab_000','cr_000'], axis = 1)\n",
        "    \n",
        "    # here we are manualy selecting the those feature name which have less than 15% missing values\n",
        "  median_data = ['aa_000', 'ac_000','ae_000','af_000','ag_001','ag_002','ag_003','ag_004','ag_005','ag_006','ag_007','ag_008','ag_009','ah_000','ai_000','aj_000','ak_000','al_000','am_0','an_000','ao_000','ap_000','aq_000','ar_000','as_000','at_000','au_000','av_000','ax_000','ay_000','ay_001','ay_002','ay_003','ay_004','ay_005','ay_006','ay_007','ay_008','ay_009','az_000','az_001','az_002','az_003','az_004','az_005','az_006','az_007','az_008','az_009','ba_000','ba_001','ba_002','ba_003','ba_004','ba_005','ba_006','ba_007','ba_008','ba_009','bb_000','bc_000','bd_000','be_000','bf_000','bg_000','bh_000','bi_000','bj_000','bs_000','bt_000','bu_000','bv_000','bx_000','by_000','bz_000','ca_000','cc_000','ce_000','ci_000','cj_000','ck_000','cn_000','cn_001','cn_002','cn_003','cn_004','cn_005','cn_006','cn_007','cn_008','cn_009','cp_000','cq_000','cs_000','cs_001','cs_002','cs_003','cs_004','cs_005','cs_006','cs_007','cs_008','cs_009','dd_000','de_000','df_000','dg_000','dh_000','di_000','dj_000','dk_000','dl_000','dm_000','dn_000','do_000','dp_000','dq_000','dr_000','ds_000','dt_000','du_000','dv_000','dx_000','dy_000','dz_000','ea_000','eb_000','ee_000','ee_001','ee_002','ee_003','ee_004','ee_005','ee_006','ee_007','ee_008','ee_009','ef_000','eg_000']\n",
        "    \n",
        "    # here we are selecting the those feature which have less than 75% and more than 15% missing values \n",
        "  model_data = ['ad_000', 'bk_000', 'bl_000', 'bm_000', 'bn_000', 'cf_000', 'cg_000','ch_000', 'cl_000', 'cm_000', 'co_000', 'ct_000', 'cu_000', 'cv_000','cx_000', 'cy_000', 'cz_000', 'da_000', 'db_000', 'dc_000', 'ec_00','ed_000']\n",
        "    \n",
        "    # here we are seprating those feature which are going to impute by median_imputation from data set\n",
        "  median_df = x.filter(median_data)\n",
        "\n",
        "    # here we are seperating the model_impute feature\n",
        "  model_df = x.filter(model_data)\n",
        "\n",
        "  median_imp = median_imputer.fit_transform(median_df)\n",
        "    \n",
        "    # making the data frame \n",
        "  median_imp_df = pd.DataFrame(median_imp, columns= median_df.columns)\n",
        "    \n",
        "    # KNN imputation \n",
        "  imputer = KNNImputer()\n",
        "  model_imp_test = imputer.fit_transform(model_df)\n",
        "  model_imp_df = pd.DataFrame(model_imp_test, columns= model_df.columns)\n",
        "    \n",
        "    # here we are concatinating the median_imputed dataframe and model_imputed data\n",
        "  data_df = pd.concat((median_imp_df, model_imp_df), axis = 1)\n",
        "  scalar =StandardScaler()\n",
        "  scalar.fit(data_df)\n",
        "  scal_data = scalar.transform(data_df)\n",
        "  scaler_data = pd.DataFrame(scal_data, columns = data_df.columns)\n",
        "\n",
        "  pca = PCA(n_components= 0.95)\n",
        "  pca_df = pca.fit_transform(scaler_data)\n",
        "  pca_df = pd.DataFrame(pca_df)\n",
        "  final_df = pd.concat((pca_df, scaler_data), axis = 1)\n",
        "\n",
        "  model = joblib.load('/content/drive/MyDrive/model_1.pkl')\n",
        "  y_pred = model.predict(final_df)\n",
        "\n",
        "  cm = confusion_matrix(y, y_pred)\n",
        "  total_cast = (cm[0][1] * 10 + cm[1][0] * 500)\n",
        "  \n",
        "\n",
        "  return total_cast\n",
        "\n"
      ],
      "metadata": {
        "id": "ODPrZlDZu7zx"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/aps_failure_test_set.csv', header = 'infer',skiprows= 20)\n",
        "remap = {'neg': 0, 'pos': 1}\n",
        "data = data.replace(remap)\n",
        "Y_test = data['class']\n",
        "X_test = data.drop('class', axis = 1)"
      ],
      "metadata": {
        "id": "JwVJe5epXtJl"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Total_cast = function_2(X_test, Y_test)\n",
        "print(\"total cast is:\",Total_cast)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfmGOprHcdvE",
        "outputId": "c171dc32-bd5d-4622-ae62-4ca115dbf3c0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total cast is: 11340\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "MinOso_hen2I"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Making_the_Most_of_your_Colab_Subscription (3).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}